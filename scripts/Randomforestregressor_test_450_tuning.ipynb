{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('../Data/ACT1_train_450.csv', dtype={\"MOLECULE\": object, \"Act\": float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinxinmo/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "y = train_1['Act'].values\n",
    "train_1 = train_1.drop(['Act', 'Unnamed: 0'], axis = 1)\n",
    "x = train_1.values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size = 0.80, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23833,) (23833, 441) (5959,) (5959, 441)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Y_train), np.shape(X_train), np.shape(Y_test), np.shape(X_test))\n",
    "Y_train = np.reshape(Y_train,(len(Y_train),1))\n",
    "Y_test = np.reshape(Y_test,(len(Y_test),1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_square(y, y_pred):\n",
    "    \"\"\" r^2 value defined by the competition host, r^2 = 1 indicates 100% prediction accuracy\n",
    "    \"\"\"\n",
    "    avx = np.mean(y)\n",
    "    avy = np.mean(y_pred)\n",
    "    sum1, sumx, sumy = 0, 0, 0\n",
    "    for i in range(len(y)):\n",
    "        sum1 += (y[i] - avx)*(y_pred[i] - avy)\n",
    "        sumx += (y[i] - avx)*(y[i] - avx)\n",
    "        sumy += (y_pred[i] - avy)*(y_pred[i] - avy)\n",
    "#     print(len(y), sum1, sumx, sumy)\n",
    "    return sum1*sum1/(sumx*sumy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE = []\n",
    "def mean_ape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true-y_pred)/y_true))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true,y_pred):\n",
    "    return np.mean(np.abs((y_true-y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#clean up output\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = RandomForestRegressor(n_estimators=500, bootstrap = True, max_features = 'sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.3003 5.6414 4.3003 ... 4.7636 4.3003 4.3003]\n"
     ]
    }
   ],
   "source": [
    "#custom R_2 score function\n",
    "from sklearn.metrics import make_scorer\n",
    "R_2 = make_scorer(r_square)\n",
    "#clean up y\n",
    "Y_train = np.ravel(Y_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67269189 0.66346275 0.62196777 0.66579094 0.66779277 0.64608175\n",
      " 0.65903882]\n",
      "The average accuracy is: 0.656689525679745\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "scores = cross_val_score(model,X_train, Y_train, cv = 7, scoring = R_2)\n",
    "print(scores)\n",
    "print('The average accuracy is (R2):', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.47174133 5.53031001 5.45231223 5.47641435 5.53473753 5.5974416\n",
      " 5.46300367]\n",
      "The average accuracy is: 5.503708673136942\n"
     ]
    }
   ],
   "source": [
    "#Mean absolute percentage error with 450 features selected with highest positive and negative correlation\n",
    "mean_ape = make_scorer(mean_ape)\n",
    "scores_mean_ape = cross_val_score(model, X_train, Y_train, cv = 7, scoring = mean_ape)\n",
    "print(scores_mean_ape)\n",
    "print('The average accuracy is:', scores_mean_ape.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26936905 0.27272948 0.26394915 0.26983211 0.27428022 0.27690475\n",
      " 0.26862467]\n",
      "The average accuracy is: 0.270812775694426\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "mae = make_scorer(mean_absolute_error)\n",
    "scores = cross_val_score(model,X_train, Y_train, cv = 7, scoring = mae)\n",
    "print(scores)\n",
    "print('The average accuracy is:', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "### Specify the domain for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "# parameters for GridSearchCV\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "# run randomized search\n",
    "n_iter_search = 150\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23833, 441)\n",
      "(23833,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=150, n_jobs=None,\n",
       "          param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4ae311ce80>, 'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4ae2e44e80>, 'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4ae2d21128>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=10):\n",
    "    f= open(\"Tuning_RF.txt\",\"w+\")\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            f.write(\"Model with rank: {0}\".format(i))\n",
    "            f.write(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            f.write(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# Utility function to report best scores\n",
    "def printReport(results, n_top=10):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.617 (std: 0.004)\n",
      "Parameters: {'max_features': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.612 (std: 0.004)\n",
      "Parameters: {'max_features': 8, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.612 (std: 0.004)\n",
      "Parameters: {'max_features': 8, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.609 (std: 0.004)\n",
      "Parameters: {'max_features': 9, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.603 (std: 0.003)\n",
      "Parameters: {'max_features': 8, 'min_samples_leaf': 1, 'min_samples_split': 6}\n",
      "\n",
      "\n",
      "Model with rank: 6\n",
      "Mean validation score: 0.603 (std: 0.004)\n",
      "Parameters: {'max_features': 10, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "\n",
      "\n",
      "Model with rank: 7\n",
      "Mean validation score: 0.603 (std: 0.005)\n",
      "Parameters: {'max_features': 8, 'min_samples_leaf': 1, 'min_samples_split': 6}\n",
      "\n",
      "\n",
      "Model with rank: 8\n",
      "Mean validation score: 0.602 (std: 0.004)\n",
      "Parameters: {'max_features': 7, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\n",
      "\n",
      "Model with rank: 9\n",
      "Mean validation score: 0.600 (std: 0.004)\n",
      "Parameters: {'max_features': 10, 'min_samples_leaf': 1, 'min_samples_split': 9}\n",
      "\n",
      "\n",
      "Model with rank: 10\n",
      "Mean validation score: 0.599 (std: 0.004)\n",
      "Parameters: {'max_features': 10, 'min_samples_leaf': 3, 'min_samples_split': 6}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printReport(random_search.cv_results_)\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66438658 0.65268734 0.61591508 0.65834815 0.65922775 0.6395571\n",
      " 0.65154725]\n",
      "The average accuracy is: 0.6488098913034642\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "tu_model = RandomForestRegressor(n_estimators=500, max_features = 10, min_samples_leaf = 1, min_samples_split = 2)\n",
    "tu_scores = cross_val_score(tu_model, X_train, Y_train, cv = 7, scoring = R_2)\n",
    "print(tu_scores)\n",
    "print('The average accuracy is:', tu_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.57058769 5.65798305 5.54688902 5.59008767 5.62644915 5.68347886\n",
      " 5.58485112]\n",
      "The average accuracy is: 5.608618080593089\n"
     ]
    }
   ],
   "source": [
    "tu_scores = cross_val_score(tu_model, X_train, Y_train, cv = 7, scoring = mean_ape)\n",
    "print(tu_scores)\n",
    "print('The average accuracy is:', tu_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27512282 0.27905111 0.26966349 0.27522263 0.27955628 0.28288828\n",
      " 0.27451251]\n",
      "The average accuracy is: 0.27657387423978175\n"
     ]
    }
   ],
   "source": [
    "tu_scores = cross_val_score(tu_model, X_train, Y_train, cv = 7, scoring = mae)\n",
    "print(tu_scores)\n",
    "print('The average accuracy is:', tu_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
